# -*- coding: utf-8 -*-
"""CSCE636_FinalProject_VGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
		https://colab.research.google.com/drive/1LvmrJgtme68W5562VMRrGTRxfcDuLlFr
"""

import os
import shutil
import numpy as np
import matplotlib.pyplot as plt
import math
import random

# Connected to localhost
data_dir = './stanford-dogs-dataset'

# # Connected to Google Drive
# from google.colab import drive
# drive.mount('/content/gdrive')
# data_dir = '/content/gdrive/My Drive/Colab Notebooks/stanford-dogs-dataset'

images_dir = os.path.join(data_dir, 'Images')
# Right now we don't need these attributes
# annotations_dir = os.path.join(data_dir, 'Annotation')

i_w = i_h = 224

def setDir(folder_name):
	global data_dir
	f_path = os.path.join(data_dir, folder_name)

	if os.path.isdir(f_path):
		# this folder already exists
		shutil.rmtree(f_path)
	os.mkdir(f_path)
	return f_path

train_dir = setDir('Train')
val_dir = setDir('Validation')
test_dir = setDir('Test')

breed_numberings = {}
train_val_test_ratio = np.array([4, 1, 1])

for content in os.listdir(images_dir):
	if content == '.DS_Store':
		continue

	# Create labels dictionary to validate the outcomes
	labels = content.split('-')
	if len(labels) < 2:
		continue
	breed_numberings[labels[0]] = labels[1]

	# Count # of images in each folder to analyze data
	folder_dir = os.path.join(images_dir, content)
	file_names = np.asarray(os.listdir(folder_dir))
	file_names = np.delete(file_names, np.argwhere(file_names == '.DS_Store'))
	file_num = len(file_names)
	np.random.shuffle(file_names)

	# Copy and create train / validation / test datasets
	file_sep = 0

	dst_dir = os.path.join(train_dir, labels[1])
	os.mkdir(dst_dir)
	train_sample_num = math.floor(file_num * train_val_test_ratio[0] / np.sum(train_val_test_ratio))
	for i in range(file_sep, file_sep + train_sample_num):
		src = os.path.join(folder_dir, file_names[i])
		dst = os.path.join(dst_dir, file_names[i])
		shutil.copyfile(src, dst)
	file_sep += train_sample_num

	dst_dir = os.path.join(val_dir, labels[1])
	os.mkdir(dst_dir)
	val_sample_num = math.floor(file_num * train_val_test_ratio[1] / np.sum(train_val_test_ratio))
	for i in range(file_sep, file_sep + val_sample_num):
		src = os.path.join(folder_dir, file_names[i])
		dst = os.path.join(dst_dir, file_names[i])
		shutil.copyfile(src, dst)
	file_sep += val_sample_num

	dst_dir = os.path.join(test_dir, labels[1])
	os.mkdir(dst_dir)
	test_sample_num = math.floor(file_num * train_val_test_ratio[2] / np.sum(train_val_test_ratio))
	for i in range(file_sep, file_sep + test_sample_num):
		src = os.path.join(folder_dir, file_names[i])
		dst = os.path.join(dst_dir, file_names[i])
		shutil.copyfile(src, dst)
	file_sep += test_sample_num

train_sample_num = 0
val_sample_num = 0
test_sample_num = 0

for f in os.listdir(train_dir):
	if f == '.DS_Store':
		continue
	f_path = os.path.join(train_dir, f)
	train_sample_num += len(os.listdir(f_path))

for f in os.listdir(val_dir):
	if f == '.DS_Store':
		continue
	f_path = os.path.join(val_dir, f)
	val_sample_num += len(os.listdir(f_path))

for f in os.listdir(test_dir):
	if f == '.DS_Store':
		continue
	f_path = os.path.join(test_dir, f)
	test_sample_num += len(os.listdir(f_path))

print(train_sample_num, val_sample_num, test_sample_num)

from keras.models import Sequential
from keras import layers, optimizers
from keras.applications import VGG16
from keras.preprocessing import image

# Hyper-parameters tuning
train_batch = 15
val_batch = 15
test_batch = 15
epoch = 50
dropout_rate = 0.4
fine_tune_layer = 'block5_conv1'
learning_rate = 2e-5

conv_base = VGG16(weights = 'imagenet', include_top = False, input_shape = (i_w, i_h, 3))

# Fine-tuning
conv_base.trainable = True
flag_trainable = False

for layer in conv_base.layers:
	if layer.name == fine_tune_layer:
		flag_trainable = True

	if flag_trainable:
		layer.trainable = True
	else:
		layer.trainable = False

conv_base.summary()

model = Sequential()

model.add(conv_base)

model.add(layers.Flatten())
model.add(layers.Dense(256, activation = 'relu'))

model.add(layers.Dropout(dropout_rate))

model.add(layers.Dense(len(breed_numberings), activation = 'softmax'))

model.summary()

# Data augmentation
train_datagen = image.ImageDataGenerator(
	rescale = 1. / 255,
	rotation_range = 30,
	width_shift_range = 0.2,
	height_shift_range = 0.2,
	shear_range = 0.2,
	zoom_range = 0.2,
	horizontal_flip = True
)

train_generator = train_datagen.flow_from_directory(train_dir, target_size = (i_w, i_h), batch_size = train_batch)

test_datagen = image.ImageDataGenerator(rescale = 1. / 255)

train_epoch_steps = math.ceil(train_sample_num / train_batch)

# val_generator = test_datagen.flow_from_directory(val_dir, target_size = (i_w, i_h), batch_size = val_batch)
# val_epoch_steps = math.ceil(val_sample_num / val_batch)

# model.compile(loss = 'categorical_crossentropy', optimizer = optimizers.RMSprop(lr = learning_rate), metrics = ['acc'])

# history = model.fit_generator(train_generator, steps_per_epoch = train_epoch_steps, epochs = epoch, validation_data = val_generator, validation_steps = val_epoch_steps)

# acc = history.history['acc']
# val_acc = history.history['val_acc']

# loss = history.history['loss']
# val_loss = history.history['val_loss']

# epochs = range(1, len(acc) + 1)

# plt.plot(epochs, acc, 'bo', label = "Training acc")
# plt.plot(epochs, val_acc, 'b', label = "Validation acc")
# plt.title("Training and Validation accuracy")
# plt.legend()

# plt.figure()

# plt.plot(epochs, loss, 'bo', label = "Training loss")
# plt.plot(epochs, val_loss, 'b', label = "Validation loss")
# plt.title("Training and Validation loss")
# plt.legend()

# plt.show()

"""### Below are training both train and validation dataset as a whole and testing."""

# Copy data from Validation into Train
for subject in os.listdir(val_dir):
	if subject == '.DS_Store':
		continue

	s_path = os.path.join(val_dir, subject)
	for f in os.listdir(s_path):
		if f == '.DS_Store':
			continue

		src = os.path.join(s_path, f)
		dst_dir = os.path.join(train_dir, subject)
		dst = os.path.join(dst_dir, f)
		shutil.copyfile(src, dst)

# Retrain the model using all training and validating data
train_generator = train_datagen.flow_from_directory(train_dir, target_size = (i_w, i_h), batch_size = train_batch)

train_epoch_steps = math.ceil((train_sample_num + val_sample_num) / train_batch)

model.compile(loss = 'categorical_crossentropy', optimizer = optimizers.RMSprop(lr = learning_rate), metrics = ['acc'])

history = model.fit_generator(train_generator, steps_per_epoch = train_epoch_steps, epochs = epoch)

model.save('/content/gdrive/My Drive/Colab Notebooks/CSCE636_final_train_test.h5')

test_generator = test_datagen.flow_from_directory(test_dir, target_size = (i_w, i_h), batch_size = test_batch)
test_epoch_steps = math.ceil(test_sample_num / test_batch)

model.evaluate_generator(test_generator, steps = test_epoch_steps)
